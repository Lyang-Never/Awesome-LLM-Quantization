# Awesome-LLM-Quantization
Awesome list of LLM Quantization

eg:
=========================================================================

- SparseGPT: Massive Language Models Can be Accurately Pruned in One-shot 
    - Label: <img src=https://img.shields.io/badge/unstructured-turquoise.svg ><img src=https://img.shields.io/badge/semi_structured-brightgreen.svg >
    - Author: Elias Frantar, Dan Alistarh
    - Link: https://arxiv.org/pdf/2301.00774.pdf 
    - Code: https://github.com/IST-DASLab/sparsegpt 
    - Pub: ICML 2023
    - Summary: First to prune GPT with at least 50% sparsity without any training. SparseGPT is entirely local, which only focus on weight updates without any global gradient information.

List:
=========================================================================

- OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models
    - Label: -
    - Author: Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok Park
    - Link: https://arxiv.org/abs/2306.02272
    - Code: -
    - Pub: -
    - Summary: -
- Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization
    - Label: -
    - Author: Seungwoo Son, Wonpyo Park, Woohyun Han, Kyuyeun Kim, Jaeho Lee / POSTECH Google
    - Link: [https://arxiv.org/abs/2306.02272](https://arxiv.org/pdf/2406.12016)
    - Code: -
    - Pub: -
    - Summary: -
